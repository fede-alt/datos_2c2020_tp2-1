{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datos_2c2020_tp2.models.log.Logger as logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../datasets/xgb-train.csv\")\n",
    "test = pd.read_csv(\"../datasets/xgb-test.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# train['Week_Day'].describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 - XGBoost Binary Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Luego verifico que dropear filas que se overlapean con test implica\n",
    "dejar de lado la mayoria de datos por lo cual no se tendraa en cuenta.\n",
    "(djo el codigo de dropeo comentaado)\n",
    "\n",
    "-----------------------------------------\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Seteo de features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features totales: 115\n"
     ]
    }
   ],
   "source": [
    "target = \"target\"\n",
    "features = list(train.columns)\n",
    "features.remove(target)\n",
    "features.remove(\"Opportunity_ID\")\n",
    "\n",
    "#VANILLA NO\n",
    "# features.remove(\"delivery_delay\")\n",
    "# features.remove(\"opportunity_lifetime\")\n",
    "# features.remove(\"converted_taxable_amount\")\n",
    "# features.remove(\"last_modified_to_delivery\")\n",
    "# features.remove(\"currency_conversion_rate\")\n",
    "# features.remove(\"Occur\")\n",
    "# features.remove(\"delivery_window\")\n",
    "# features.remove(\"account_creation_to_created_opp\")\n",
    "\n",
    "#VANILLA SI\n",
    "features.remove('Total_Taxable_Amount')\n",
    "features.remove('ASP_(converted)')\n",
    "features.remove(\"ASP\")\n",
    "features.remove(\"Total_Amount\")\n",
    "features.remove(\"Delivery_Year\")\n",
    "# features.remove(\"Week_Day\")\n",
    "\n",
    "#CONSIDERO REMOVIBLES\n",
    "# features.remove(\"created_blocknum\")   #FECHA!!\n",
    "features.remove(\"late_delivery_blocknum\")\n",
    "features.remove(\"early_delivery_blocknum\")\n",
    "features.remove(\"last_modified_blocknum\")\n",
    "features.remove(\"account_creation_blocknum\") #OJO CCON HIPOTESIS DEL CLIENTE VIEEJO\n",
    "print(\"Features totales: {}\".format(len(features)))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Armado de sets\n",
    "FORMAS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set de entrenamiento (size) (16947, 115)\n",
      "Set de testing (size) (2551, 115)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = train.loc[:,features], train.loc[:,target]\n",
    "X_test_Opp = test.loc[:,\"Opportunity_ID\"]\n",
    "X_test = test.loc[:,features]\n",
    "\n",
    "print(\"Set de entrenamiento (size) {}\".format(X_train.shape))\n",
    "print(\"Set de testing (size) {}\".format(X_test.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Por otra parte seepaaro el test de entrenamiento en aprox~(80%/20%) segun la fecha de creeacion de la oportunidad."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "percent_80 = train[\"created_blocknum\"].describe()['75%']/0.945\n",
    "beta_test = train[train[\"created_blocknum\"]>percent_80]\n",
    "beta_train = train[train[\"created_blocknum\"]<percent_80]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set de entrenamiento secundario (size) (14078, 115)\n",
      "Set de testing secundario (size) (2869, 115)\n"
     ]
    }
   ],
   "source": [
    "A_train, b_train = beta_train.loc[:,features], beta_train.loc[:,target]\n",
    "# A_test_Opp = beta_test.loc[:,\"Opportunity_ID\"]\n",
    "A_test, b_test = beta_test.loc[:,features], beta_test.loc[:,target]\n",
    "\n",
    "print(\"Set de entrenamiento secundario (size) {}\".format(A_train.shape))\n",
    "print(\"Set de testing secundario (size) {}\".format(A_test.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tuneo de hiperparametros\n",
    "* GRIDSEARCH + CROSS VALIDATION\n",
    "\n",
    "Configuracion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando nfolds=7,  num_boost_round=160  y  early_stopping_rounds=10\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------\n",
    "#----------------------------- C O N F I G U R A C I O N ----------------------------------\n",
    "#------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "nfolds = 7\n",
    "rounds = 160\n",
    "early_stopping_rounds = 10\n",
    "\n",
    "params = {\"objective\": \"binary:logistic\",\n",
    "          'colsample_bytree': 0.3,\n",
    "          'subsample': 1.0,\n",
    "          'learning_rate': 0.1,\n",
    "          'max_depth': 6,\n",
    "          'min_child_weight': 1,\n",
    "          'eval_metric': 'logloss'}\n",
    "beta_params = params.copy()\n",
    "beta_params['n_estimators'] = 100\n",
    "\n",
    "\n",
    "enaable_gridsearch_for_tree = True\n",
    "enaable_gridsearch_for_sampling = True\n",
    "enaable_gridsearch_for_learning = True\n",
    "final_cv = True\n",
    "enable_parcial_training = True\n",
    "\n",
    "print(\"Usando nfolds={},  num_boost_round={}\"\n",
    "      \"  y  early_stopping_rounds={}\".format(nfolds, rounds, early_stopping_rounds))\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#FUNCIONES\n",
    "\n",
    "def find_best_param_tuple(dtrain, params, param_tuple):\n",
    "    # Define initial best params and LogLoss\n",
    "    min_logloss = float(\"Inf\")\n",
    "    best_params = None\n",
    "    for param0, param1 in param_tuple['grid']:\n",
    "        print(\"CV with {}={}, {}={}\".format(param_tuple['names'][0],\n",
    "                                 param0,\n",
    "                                 param_tuple['names'][1],\n",
    "                                 param1))\n",
    "        # Update our parameters\n",
    "        params[param_tuple['names'][0]] = param0\n",
    "        if param_tuple['names'][1]: params[param_tuple['names'][1]] = param1\n",
    "        # Run CV\n",
    "        cv_results = xgb.cv(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=rounds,\n",
    "            seed=1234,\n",
    "            nfold=nfolds,\n",
    "            metrics='logloss',\n",
    "            early_stopping_rounds=early_stopping_rounds\n",
    "        )\n",
    "        # Update best LogLoss\n",
    "        mean_logloss = cv_results['test-logloss-mean'].min()\n",
    "        boost_rounds = cv_results['test-logloss-mean'].argmin()\n",
    "        print(\"\\tLL {} for {} rounds\".format(mean_logloss, boost_rounds+1))\n",
    "        if mean_logloss < min_logloss:\n",
    "            min_logloss = mean_logloss\n",
    "            best_params = (param0, param1)\n",
    "    print(\"Best {},{}: {}, {}, LogLoss: {}\".format(param_tuple['names'][0],param_tuple['names'][1],best_params[0], best_params[1], min_logloss))\n",
    "    return best_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TUNEO DE:\n",
    "* max_depth\n",
    "* min_child_weight"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with max_depth=20, min_child_weight=1\n",
      "\tLL 0.15717857142857142 for 135 rounds\n",
      "CV with max_depth=21, min_child_weight=1\n",
      "\tLL 0.156558 for 157 rounds\n",
      "CV with max_depth=22, min_child_weight=1\n",
      "\tLL 0.15760085714285715 for 148 rounds\n",
      "CV with max_depth=23, min_child_weight=1\n",
      "\tLL 0.15631257142857144 for 132 rounds\n",
      "CV with max_depth=24, min_child_weight=1\n",
      "\tLL 0.1575902857142857 for 148 rounds\n",
      "Best max_depth,min_child_weight: 23, 1, LogLoss: 0.15631257142857144\n"
     ]
    }
   ],
   "source": [
    "if enaable_gridsearch_for_tree:\n",
    "\n",
    "    tree_params_tuple = {\n",
    "        'names': ('max_depth', 'min_child_weight'),\n",
    "        'grid': [\n",
    "            (max_depth, min_child_weight)\n",
    "            for max_depth in range(20,26,1)\n",
    "            for min_child_weight in range(1,2)\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    best_tree_params = find_best_param_tuple(dtrain, params, tree_params_tuple)\n",
    "\n",
    "    params['max_depth'] = best_tree_params[0]\n",
    "    params['min_child_weight'] = best_tree_params[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TUNEO DE:\n",
    "* subsamble\n",
    "* colsample_bytree\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLL 0.31050628 for 50 rounds\n",
      "CV with subsample=0.9, colsample_bytree=0.3\n",
      "\tLL 0.31557734 for 50 rounds\n",
      "CV with subsample=0.9, colsample_bytree=0.4\n",
      "\tLL 0.31073799999999996 for 50 rounds\n",
      "CV with subsample=1.0, colsample_bytree=0.3\n",
      "\tLL 0.31559950000000003 for 50 rounds\n",
      "CV with subsample=1.0, colsample_bytree=0.4\n",
      "\tLL 0.31125966 for 50 rounds\n",
      "Best subsample,colsample_bytree: 0.8, 0.4, LogLoss: 0.31050628\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with subsample=0.8, colsample_bytree=0.3\n",
      "\tLL 0.16312914285714286 for 149 rounds\n",
      "CV with subsample=0.8, colsample_bytree=0.4\n",
      "\tLL 0.16095371428571428 for 155 rounds\n",
      "CV with subsample=0.9, colsample_bytree=0.3\n",
      "\tLL 0.1595277142857143 for 159 rounds\n",
      "CV with subsample=0.9, colsample_bytree=0.4\n",
      "\tLL 0.1609108571428571 for 127 rounds\n",
      "CV with subsample=1.0, colsample_bytree=0.3\n",
      "\tLL 0.15631257142857144 for 132 rounds\n",
      "CV with subsample=1.0, colsample_bytree=0.4\n",
      "\tLL 0.15859299999999998 for 128 rounds\n",
      "Best subsample,colsample_bytree: 1.0, 0.3, LogLoss: 0.15631257142857144\n"
     ]
    }
   ],
   "source": [
    "if enaable_gridsearch_for_sampling:\n",
    "\n",
    "    sample_params_tuple = {\n",
    "        'names': ('subsample', 'colsample_bytree'),\n",
    "        'grid': [\n",
    "            (subsample, colsample_bytree)\n",
    "            for subsample in [i/10. for i in range(8,11)]\n",
    "            for colsample_bytree in  [i/10. for i in range(3,5)]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    best_sample_params = find_best_param_tuple(dtrain, params, sample_params_tuple)\n",
    "\n",
    "    params['subsample'] = best_sample_params[0]\n",
    "    params['colsample_bytree'] = best_sample_params[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TUNEO DE:\n",
    "* learning rate (eta)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with learning_rate=0.2, =1\n",
      "\tLL 0.1620764285714286 for 70 rounds\n",
      "CV with learning_rate=0.15, =1\n",
      "\tLL 0.15898828571428572 for 109 rounds\n",
      "CV with learning_rate=0.1, =1\n",
      "\tLL 0.15631257142857144 for 132 rounds\n",
      "CV with learning_rate=0.05, =1\n",
      "\tLL 0.1615062857142857 for 160 rounds\n",
      "Best learning_rate,: 0.1, 1, LogLoss: 0.15631257142857144\n"
     ]
    }
   ],
   "source": [
    "if enaable_gridsearch_for_learning:\n",
    "    learning_rate_grid = [\n",
    "        (learning_rate, empty)\n",
    "        for learning_rate in [0.2, 0.15, 0.1, 0.05]\n",
    "        for empty in range(1,2)\n",
    "    ]\n",
    "    learning_rate = {\n",
    "        'names': ('learning_rate',''),\n",
    "        'grid': learning_rate_grid,\n",
    "    }\n",
    "    params['learning_rate'] = find_best_param_tuple(dtrain, params, learning_rate)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Final cross validation error"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rounds    LogLoss \n",
      "  175    0.147247\n",
      "Name: test-logloss-mean, dtype: float64.\n"
     ]
    },
    {
     "data": {
      "text/plain": "     train-logloss-mean  train-logloss-std  test-logloss-mean  \\\n175             0.01203           0.000132           0.147247   \n\n     test-logloss-std  \n175          0.047316  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>train-logloss-mean</th>\n      <th>train-logloss-std</th>\n      <th>test-logloss-mean</th>\n      <th>test-logloss-std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>175</th>\n      <td>0.01203</td>\n      <td>0.000132</td>\n      <td>0.147247</td>\n      <td>0.047316</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if final_cv:\n",
    "    cv_results = xgb.cv(\n",
    "                params,\n",
    "                dtrain,\n",
    "                num_boost_round=1000,\n",
    "                seed=1234,\n",
    "                nfold=len(features),\n",
    "                metrics='logloss',\n",
    "                early_stopping_rounds=early_stopping_rounds\n",
    "            )\n",
    "    print(\"Rounds    LogLoss \\n  {}.\".format(cv_results[\"test-logloss-mean\"].tail(1), cv_results[\"test-logloss-mean\"].tail(1)))\n",
    "display(cv_results.tail(1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Entrenamiento"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.3, eval_metric='logloss',\n              gamma=0, gpu_id=-1, importance_type='gain',\n              interaction_constraints='', learning_rate=0.1, max_delta_step=0,\n              max_depth=23, min_child_weight=1, missing=nan,\n              monotone_constraints='()', n_estimators=175, n_jobs=8,\n              num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1,\n              scale_pos_weight=1, subsample=1.0, tree_method='exact',\n              use_label_encoder=False, validate_parameters=1, verbosity=None)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf = xgb.XGBClassifier(objective ='binary:logistic',\n",
    "            eval_metric='logloss',\n",
    "            use_label_encoder=False,\n",
    "            colsample_bytree = params['colsample_bytree'],\n",
    "            subsample = params['subsample'],\n",
    "            learning_rate = params['learning_rate'],\n",
    "            max_depth = params['max_depth'],\n",
    "            n_estimators = 175,\n",
    "            reg_alpha=0,\n",
    "            min_child_weight = params['min_child_weight'])\n",
    "\n",
    "\n",
    "xgb_clf.fit(X_train,y_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "preds = [pred[1] for pred in xgb_clf.predict_proba(X_test)]\n",
    "# preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ENTRENAMIENTO PARCIAL\n",
    "CALCULO DE ERROR EN BASE A PARTICION DE SET DE ENTRENAMIENTO POR FECHA DE CREACION DE LA OPORTUNIDAD"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TUNEANDO alpha \n",
      "\n",
      "alpha=0, =1\n",
      "Log loss: 0.294366705810412\n",
      "Accuracy: 87.31%\n",
      "alpha=1, =1\n",
      "Log loss: 0.2972205101207274\n",
      "Accuracy: 87.10%\n",
      "alpha=2, =1\n",
      "Log loss: 0.30334033257717596\n",
      "Accuracy: 86.72%\n",
      "Best ('alpha', ''): (0, 1) with LogLoss: 0.294366705810412\n",
      "Best ('alpha', ''): (0, 1) with Accu: 0.8731265249215755\n",
      "\n",
      " TUNEANDO learning_rate \n",
      "\n",
      "learning_rate=0.14, =1\n",
      "Log loss: 0.29744629388755367\n",
      "Accuracy: 88.01%\n",
      "learning_rate=0.13, =1\n",
      "Log loss: 0.31371784601821007\n",
      "Accuracy: 87.00%\n",
      "learning_rate=0.12, =1\n",
      "Log loss: 0.3042819112856431\n",
      "Accuracy: 87.49%\n",
      "learning_rate=0.11, =1\n",
      "Log loss: 0.3009535538540431\n",
      "Accuracy: 86.75%\n",
      "learning_rate=0.1, =1\n",
      "Log loss: 0.3066552127427132\n",
      "Accuracy: 86.48%\n",
      "learning_rate=0.09, =1\n",
      "Log loss: 0.294366705810412\n",
      "Accuracy: 87.31%\n",
      "learning_rate=0.08, =1\n",
      "Log loss: 0.30074054923240223\n",
      "Accuracy: 87.31%\n",
      "Best ('learning_rate', ''): (0.09, 1) with LogLoss: 0.294366705810412\n",
      "Best ('learning_rate', ''): (0.14, 1) with Accu: 0.8800975949808295\n",
      "\n",
      " TUNEANDO n_estimators \n",
      "\n",
      "n_estimators=130, =1\n",
      "Log loss: 0.29595860766125426\n",
      "Accuracy: 87.28%\n",
      "n_estimators=132, =1\n",
      "Log loss: 0.2954177544948912\n",
      "Accuracy: 87.31%\n",
      "n_estimators=134, =1\n",
      "Log loss: 0.2949037715351097\n",
      "Accuracy: 87.31%\n",
      "n_estimators=136, =1\n",
      "Log loss: 0.294366705810412\n",
      "Accuracy: 87.31%\n",
      "n_estimators=138, =1\n",
      "Log loss: 0.29546765712276774\n",
      "Accuracy: 87.31%\n",
      "n_estimators=140, =1\n",
      "Log loss: 0.2953315907889743\n",
      "Accuracy: 87.21%\n",
      "n_estimators=142, =1\n",
      "Log loss: 0.2946358401932699\n",
      "Accuracy: 87.31%\n",
      "n_estimators=144, =1\n",
      "Log loss: 0.2945405290137948\n",
      "Accuracy: 87.35%\n",
      "n_estimators=146, =1\n",
      "Log loss: 0.29559408563810685\n",
      "Accuracy: 87.31%\n",
      "n_estimators=148, =1\n",
      "Log loss: 0.29574267473365995\n",
      "Accuracy: 87.38%\n",
      "Best ('n_estimators', ''): (136, 1) with LogLoss: 0.294366705810412\n",
      "Best ('n_estimators', ''): (148, 1) with Accu: 0.8738236319275009\n",
      "\n",
      " TUNEANDO TREE_PARAMS \n",
      "\n",
      "max_depth=5, min_child_weight=1\n",
      "Log loss: 0.29508964142779737\n",
      "Accuracy: 87.17%\n",
      "max_depth=5, min_child_weight=2\n",
      "Log loss: 0.29846338594286326\n",
      "Accuracy: 86.79%\n",
      "max_depth=5, min_child_weight=3\n",
      "Log loss: 0.29506593126630254\n",
      "Accuracy: 86.93%\n",
      "max_depth=5, min_child_weight=4\n",
      "Log loss: 0.2953621113453676\n",
      "Accuracy: 87.10%\n",
      "max_depth=6, min_child_weight=1\n",
      "Log loss: 0.2975010038880496\n",
      "Accuracy: 87.38%\n",
      "max_depth=6, min_child_weight=2\n",
      "Log loss: 0.29697454551493074\n",
      "Accuracy: 87.49%\n",
      "max_depth=6, min_child_weight=3\n",
      "Log loss: 0.294366705810412\n",
      "Accuracy: 87.31%\n",
      "max_depth=6, min_child_weight=4\n",
      "Log loss: 0.3026806045634714\n",
      "Accuracy: 86.72%\n",
      "max_depth=7, min_child_weight=1\n",
      "Log loss: 0.2988395223281496\n",
      "Accuracy: 87.17%\n",
      "max_depth=7, min_child_weight=2\n",
      "Log loss: 0.2972504016267264\n",
      "Accuracy: 87.38%\n",
      "max_depth=7, min_child_weight=3\n",
      "Log loss: 0.2943129345886176\n",
      "Accuracy: 87.45%\n",
      "max_depth=7, min_child_weight=4\n",
      "Log loss: 0.3011712188519897\n",
      "Accuracy: 87.28%\n",
      "max_depth=8, min_child_weight=1\n",
      "Log loss: 0.3022239209170125\n",
      "Accuracy: 86.65%\n",
      "max_depth=8, min_child_weight=2\n",
      "Log loss: 0.3006081081152289\n",
      "Accuracy: 87.14%\n",
      "max_depth=8, min_child_weight=3\n",
      "Log loss: 0.30668188064562746\n",
      "Accuracy: 87.42%\n",
      "max_depth=8, min_child_weight=4\n",
      "Log loss: 0.305644720769398\n",
      "Accuracy: 86.65%\n",
      "Best ('max_depth', 'min_child_weight'): (7, 3) with LogLoss: 0.2943129345886176\n",
      "Best ('max_depth', 'min_child_weight'): (6, 2) with Accu: 0.874869292436389\n",
      "\n",
      " TUNEANDO sample_params \n",
      "\n",
      "subsample=0.6, colsample_bytree=0.1\n",
      "Log loss: 0.3605922320328115\n",
      "Accuracy: 83.62%\n",
      "subsample=0.6, colsample_bytree=0.2\n",
      "Log loss: 0.3252260303947179\n",
      "Accuracy: 85.29%\n",
      "subsample=0.6, colsample_bytree=0.3\n",
      "Log loss: 0.31070598061099325\n",
      "Accuracy: 86.55%\n",
      "subsample=0.6, colsample_bytree=0.4\n",
      "Log loss: 0.3187675408497234\n",
      "Accuracy: 86.23%\n",
      "subsample=0.7, colsample_bytree=0.1\n",
      "Log loss: 0.3558086178679559\n",
      "Accuracy: 83.58%\n",
      "subsample=0.7, colsample_bytree=0.2\n",
      "Log loss: 0.3278855340358999\n",
      "Accuracy: 86.23%\n",
      "subsample=0.7, colsample_bytree=0.3\n",
      "Log loss: 0.30510824617333115\n",
      "Accuracy: 87.28%\n",
      "subsample=0.7, colsample_bytree=0.4\n",
      "Log loss: 0.3041221717856904\n",
      "Accuracy: 87.14%\n",
      "subsample=0.8, colsample_bytree=0.1\n",
      "Log loss: 0.3571437516210113\n",
      "Accuracy: 84.11%\n",
      "subsample=0.8, colsample_bytree=0.2\n",
      "Log loss: 0.3293432044530177\n",
      "Accuracy: 86.09%\n",
      "subsample=0.8, colsample_bytree=0.3\n",
      "Log loss: 0.308704191581595\n",
      "Accuracy: 87.24%\n",
      "subsample=0.8, colsample_bytree=0.4\n",
      "Log loss: 0.30697513301129475\n",
      "Accuracy: 87.24%\n",
      "subsample=0.9, colsample_bytree=0.1\n",
      "Log loss: 0.35274451225208825\n",
      "Accuracy: 84.45%\n",
      "subsample=0.9, colsample_bytree=0.2\n",
      "Log loss: 0.3275730460322529\n",
      "Accuracy: 85.12%\n",
      "subsample=0.9, colsample_bytree=0.3\n",
      "Log loss: 0.3030282072374457\n",
      "Accuracy: 87.28%\n",
      "subsample=0.9, colsample_bytree=0.4\n",
      "Log loss: 0.3059324926557478\n",
      "Accuracy: 87.00%\n",
      "subsample=1.0, colsample_bytree=0.1\n",
      "Log loss: 0.36559199951366744\n",
      "Accuracy: 84.42%\n",
      "subsample=1.0, colsample_bytree=0.2\n",
      "Log loss: 0.31949303902452486\n",
      "Accuracy: 87.10%\n",
      "subsample=1.0, colsample_bytree=0.3\n",
      "Log loss: 0.2943129345886176\n",
      "Accuracy: 87.45%\n",
      "subsample=1.0, colsample_bytree=0.4\n",
      "Log loss: 0.3033171752601047\n",
      "Accuracy: 86.82%\n",
      "Best ('subsample', 'colsample_bytree'): (1.0, 0.3) with LogLoss: 0.2943129345886176\n",
      "Best ('subsample', 'colsample_bytree'): (1.0, 0.3) with Accu: 0.8745207389334263\n"
     ]
    }
   ],
   "source": [
    "def partial_tune(params, params_to_tune):\n",
    "    min_logloss = float(\"Inf\")\n",
    "    max_accuracy = -float(\"Inf\")\n",
    "    best_params = None\n",
    "    for param0, param1 in params_to_tune['range']:\n",
    "        print(\"{}={}, {}={}\".format(params_to_tune['names'][0],\n",
    "                                    param0,\n",
    "                                    params_to_tune['names'][1],\n",
    "                                    param1))\n",
    "        # Update our parameters\n",
    "        params[params_to_tune['names'][0]] = param0\n",
    "        params[params_to_tune['names'][1]] = param1\n",
    "        # Run CV\n",
    "        beta_xgb_clf = xgb.XGBClassifier(objective ='binary:logistic', eval_metric='logloss', use_label_encoder=False,\n",
    "                    colsample_bytree = params['colsample_bytree'],\n",
    "                    subsample = params['subsample'],\n",
    "                    learning_rate = params['learning_rate'],\n",
    "                    max_depth = int(params['max_depth']),\n",
    "                    n_estimators = params['n_estimators'],\n",
    "                    reg_alpha=params['alpha'],\n",
    "                    min_child_weight = params['min_child_weight'])\n",
    "        beta_xgb_clf.fit(A_train,b_train)\n",
    "        beta_preds = [pred[1] for pred in beta_xgb_clf.predict_proba(A_test)]\n",
    "        print(\"Log loss: {}\".format(log_loss(b_test, beta_preds)))\n",
    "\n",
    "        predictions = [round(value) for value in beta_preds]\n",
    "        accuracy = accuracy_score(b_test, predictions)\n",
    "        print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "        # Update best LogLoss\n",
    "        mean_logloss = log_loss(b_test, beta_preds)\n",
    "        if mean_logloss < min_logloss:\n",
    "            min_logloss = mean_logloss\n",
    "            best_params = (param0, param1)\n",
    "        if max_accuracy < accuracy:\n",
    "            max_accuracy = accuracy\n",
    "            best_acu_params = (param0, param1)\n",
    "    print(\"Best {}: ({}, {}) with LogLoss: {}\".format(params_to_tune['names'], best_params[0], best_params[1], min_logloss))\n",
    "    print(\"Best {}: ({}, {}) with Accu: {}\".format(params_to_tune['names'], best_acu_params[0], best_acu_params[1], max_accuracy))\n",
    "    return best_params\n",
    "\n",
    "#-----------------------------------\n",
    "#-----------------------------------\n",
    "#-----------------------------------\n",
    "\n",
    "if enable_parcial_training:\n",
    "    print(\"\\n TUNEANDO alpha \\n\")\n",
    "    aplha_range = [\n",
    "        (alpha, n_estimator)\n",
    "        for alpha in range(0,3)\n",
    "        for n_estimator in range(1,2)\n",
    "    ]\n",
    "    best_alpha = partial_tune(beta_params, {'names':('alpha',''),'range':aplha_range})\n",
    "    beta_params['alpha'] = best_alpha[0]\n",
    "    # beta_params['n_estimators'] = best_alpha[1]\n",
    "\n",
    "    print(\"\\n TUNEANDO learning_rate \\n\")\n",
    "\n",
    "    learning_rate_range = [\n",
    "        (learning_rate, empty)\n",
    "        for learning_rate in [0.14, 0.13, 0.12, 0.11, 0.10, 0.09, 0.08]\n",
    "        for empty in range(1,2)\n",
    "    ]\n",
    "    best_learning_rate = partial_tune(beta_params, {'names':('learning_rate',''),'range':learning_rate_range})\n",
    "    beta_params['learning_rate'] = best_learning_rate[0]\n",
    "\n",
    "    print(\"\\n TUNEANDO n_estimators \\n\")\n",
    "    n_estimator_range = [\n",
    "        (n_estimator, empty)\n",
    "        for n_estimator in range(130,150,2)\n",
    "        for empty in range(1,2)\n",
    "    ]\n",
    "    best_n_estimator = partial_tune(beta_params, {'names':('n_estimators',''),'range':n_estimator_range})\n",
    "    beta_params['n_estimators'] = best_n_estimator[0]\n",
    "\n",
    "\n",
    "    print(\"\\n TUNEANDO TREE_PARAMS \\n\")\n",
    "    tree_params_range = [\n",
    "        (max_depth, min_child_weight)\n",
    "        for max_depth in range(5,9)\n",
    "        for min_child_weight in range(1,5)\n",
    "    ]\n",
    "    best_tree_params = partial_tune(beta_params, {'names':('max_depth','min_child_weight'),'range':tree_params_range})\n",
    "    beta_params['max_depth'] = int(best_tree_params[0])\n",
    "    beta_params['min_child_weight'] = int(best_tree_params[1])\n",
    "\n",
    "\n",
    "    print(\"\\n TUNEANDO sample_params \\n\")\n",
    "    sample_grid_params = [\n",
    "        (subsample, colsample_bytree)\n",
    "        for subsample in [i/10. for i in range(6,11)]\n",
    "        for colsample_bytree in  [i/10. for i in range(1,5)]\n",
    "    ]\n",
    "    best_sample_params = partial_tune(beta_params, {'names':('subsample','colsample_bytree'),'range':sample_grid_params})\n",
    "    beta_params['subsample'] = best_sample_params[0]\n",
    "    beta_params['colsample_bytree'] = best_sample_params[1]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=0.3,\n",
      "              eval_metric='logloss', gamma=None, gpu_id=None,\n",
      "              importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=0.09, max_delta_step=None, max_depth=7,\n",
      "              min_child_weight=3, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=136, n_jobs=None, num_parallel_tree=None,\n",
      "              random_state=None, reg_alpha=0, reg_lambda=None,\n",
      "              scale_pos_weight=None, subsample=1.0, tree_method=None,\n",
      "              use_label_encoder=False, validate_parameters=None,\n",
      "              verbosity=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": "      Opportunity_ID    Target\n0              10689  0.966305\n3              10690  0.693322\n8              10691  0.390785\n9              10692  0.359057\n15             10693  0.974250\n...              ...       ...\n2545           12364  0.966379\n2547           12365  0.247117\n2548           12366  0.172353\n2549           12367  0.659700\n2550           12368  0.101691\n\n[1567 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Opportunity_ID</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10689</td>\n      <td>0.966305</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10690</td>\n      <td>0.693322</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>10691</td>\n      <td>0.390785</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10692</td>\n      <td>0.359057</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>10693</td>\n      <td>0.974250</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2545</th>\n      <td>12364</td>\n      <td>0.966379</td>\n    </tr>\n    <tr>\n      <th>2547</th>\n      <td>12365</td>\n      <td>0.247117</td>\n    </tr>\n    <tr>\n      <th>2548</th>\n      <td>12366</td>\n      <td>0.172353</td>\n    </tr>\n    <tr>\n      <th>2549</th>\n      <td>12367</td>\n      <td>0.659700</td>\n    </tr>\n    <tr>\n      <th>2550</th>\n      <td>12368</td>\n      <td>0.101691</td>\n    </tr>\n  </tbody>\n</table>\n<p>1567 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "beta_xgb_clf = xgb.XGBClassifier(objective ='binary:logistic',\n",
    "                                eval_metric='logloss',\n",
    "                                use_label_encoder=False,\n",
    "                                colsample_bytree = beta_params['colsample_bytree'],\n",
    "                                subsample = beta_params['subsample'],\n",
    "                                learning_rate = beta_params['learning_rate'],\n",
    "                                max_depth = int(beta_params['max_depth']),\n",
    "                                n_estimators = beta_params['n_estimators'],\n",
    "                                reg_alpha=beta_params['alpha'],\n",
    "                                min_child_weight = beta_params['min_child_weight'])\n",
    "print(beta_xgb_clf)\n",
    "beta_xgb_clf.fit(X_train,y_train)\n",
    "beta_preds = [pred[1] for pred in beta_xgb_clf.predict_proba(X_test)]\n",
    "beta_pred_df = pd.DataFrame(X_test_Opp)\n",
    "beta_pred_df[\"Target\"] = beta_preds\n",
    "beta_pred_df = beta_pred_df.drop_duplicates(\"Opportunity_ID\")\n",
    "beta_pred_df.to_csv(\"../submits/xgb-clf-tunned-beta.csv\", index=False)\n",
    "display(beta_pred_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Guardando las predicciones a CSV"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "      Opportunity_ID    Target\n0              10689  0.966765\n3              10690  0.677587\n8              10691  0.502843\n9              10692  0.242120\n15             10693  0.980258\n...              ...       ...\n2545           12364  0.981698\n2547           12365  0.345006\n2548           12366  0.176371\n2549           12367  0.734186\n2550           12368  0.037911\n\n[1567 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Opportunity_ID</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10689</td>\n      <td>0.966765</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10690</td>\n      <td>0.677587</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>10691</td>\n      <td>0.502843</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10692</td>\n      <td>0.242120</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>10693</td>\n      <td>0.980258</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2545</th>\n      <td>12364</td>\n      <td>0.981698</td>\n    </tr>\n    <tr>\n      <th>2547</th>\n      <td>12365</td>\n      <td>0.345006</td>\n    </tr>\n    <tr>\n      <th>2548</th>\n      <td>12366</td>\n      <td>0.176371</td>\n    </tr>\n    <tr>\n      <th>2549</th>\n      <td>12367</td>\n      <td>0.734186</td>\n    </tr>\n    <tr>\n      <th>2550</th>\n      <td>12368</td>\n      <td>0.037911</td>\n    </tr>\n  </tbody>\n</table>\n<p>1567 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_df = pd.DataFrame(X_test_Opp)\n",
    "pred_df[\"Target\"] = preds\n",
    "pred_df = pred_df.drop_duplicates(\"Opportunity_ID\")\n",
    "display(pred_df)\n",
    "pred_df.to_csv(\"../submits/xgb-clf-cv_tunned.csv\",index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(params)\n",
    "plt.rcParams['figure.figsize'] = [15, 25]\n",
    "xgb.plot_importance(xgb_clf)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# logger.Logger.log_model(\"xgb-logloss\", params, features, cv_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}