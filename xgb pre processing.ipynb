{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_columns = [\n",
    "                'Account_Created_Date', 'Opportunity_Created_Date',\n",
    "                'Quote_Expiry_Date', 'Last_Modified_Date',\n",
    "                'Planned_Delivery_Start_Date','Planned_Delivery_End_Date',\n",
    "                ]\n",
    "\n",
    "categorical_columns = [\n",
    "        'Region', 'Territory', 'Bureaucratic_Code',\n",
    "        'Source ', 'Billing_Country', 'Account_Name',\n",
    "        'Opportunity_Name', 'Account_Owner', 'Opportunity_Owner',\n",
    "        'Account_Type', 'Opportunity_Type', 'Quote_Type',\n",
    "        'Delivery_Terms', 'Brand', 'Product_Type',\n",
    "        'Size', 'Product_Category_B', 'Currency',\n",
    "        'Last_Modified_By', 'Product_Family', 'Product_Name',\n",
    "        'ASP_Currency', 'ASP_(converted)_Currency', 'Delivery_Quarter',\n",
    "        'Total_Amount_Currency', 'Total_Taxable_Amount_Currency', 'Stage',\n",
    "        'Prod_Category_A'\n",
    "    ]\n",
    "\n",
    "# Columnnas excluidas porque tienen igual valor en todos sus registros\n",
    "empty = ['Actual_Delivery_Date', 'Last_Activity',\n",
    "        'Submitted_for_Approval','Prod_Category_A', 'Sales_Contract_No']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def open_and_filter(dataset):\n",
    "    \n",
    "    column_types = { col:'category' for col in categorical_columns }\n",
    "    \n",
    "\n",
    "    \n",
    "    # read_csv\n",
    "    df = pd.read_csv(dataset, parse_dates=date_columns, dtype=column_types,\n",
    "                     index_col='ID', na_values=['Other', 'NaT', 'None'],\n",
    "                     usecols=lambda x: x not in empty)\n",
    "\n",
    "\n",
    "    # Re-typing\n",
    "    df['Month'] = pd.to_datetime(df['Month'], format='%Y - %m')\n",
    "\n",
    "    # Agruping regions (ME esta en EMEA)\n",
    "    df.loc[((df.Region == \"Middle East\")), \"Region\"] = \"EMEA\"\n",
    "    df.Region.cat.remove_categories('Middle East', inplace=True)\n",
    "\n",
    "    df.Territory.cat.add_categories('Other', inplace=True)\n",
    "    df.Territory.fillna('Other', inplace=True)\n",
    "    # df.loc[(df.Territory == \"None\"), \"Territory\"] = \"Other\"\n",
    "    df.loc[(df.Territory.str.contains(\"America\")), \"Region\"] = \"Americas\"\n",
    "    return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-procesamiento de los datos\n",
    "\n",
    "El objetivo de esta etapa, es recibir los datos \"crudos\" y realizar procedimientos necesarios para filtrar features de poco valor y crear otros features que revelen información de importancia, para que los modelos de machine learning que luego los utilizarán en una etapa posterior, puedan ralizar un predicción mas precisa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Fechas --> blocknumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "outputs": [],
   "source": [
    "df1 = open_and_filter(\"rawdata/Train_TP2_Datos_2020-2C.csv\")\n",
    "df2 = open_and_filter(\"rawdata/Test_TP2_Datos_2020-2C.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Separo fecha en month y year (TODOS son en el primer dia del mes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "outputs": [],
   "source": [
    "FIRST_OPP = df1[\"Opportunity_Created_Date\"].min()\n",
    "\n",
    "def days_to(x):\n",
    "    return (pd.to_datetime(x) - FIRST_OPP).days\n",
    "\n",
    "\n",
    "#blocknum\n",
    "df1['blocknum'] = df1[\"Opportunity_Created_Date\"].apply(lambda x: days_to(x))\n",
    "df2['blocknum'] = df2[\"Opportunity_Created_Date\"].apply(lambda x: days_to(x))\n",
    "\n",
    "\n",
    "\n",
    "df1['Month'] = pd.to_datetime(df1['Month']).dt.month\n",
    "df2['Month'] = pd.to_datetime(df2['Month']).dt.month\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train set has 16947 elements and 47 features\n",
      "The train set has 2551 elements and 46 features\n"
     ]
    }
   ],
   "source": [
    "print(f\"The train set has {df1.shape[0]} elements and {df1.shape[1]} features\")\n",
    "print(f\"The train set has {df2.shape[0]} elements and {df2.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Features inteeresantes\n",
    "\n",
    "Veces que aparece un opportunity_ID precios caantidades taxes etc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "outputs": [],
   "source": [
    "#ocurrencia\n",
    "df1['Occur'] = df1['Opportunity_ID'].apply(lambda x: (df1['Opportunity_ID'] == x).sum())\n",
    "df2['Occur'] = df2['Opportunity_ID'].apply(lambda x: (df2['Opportunity_ID'] == x).sum())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dropeo stages != lost y won"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "outputs": [],
   "source": [
    "valid_targets = [\"Closed_Won\", \"Closen_Lost\"]\n",
    "\n",
    "df1.drop(df1[df1[\"Stage\"].isin(valid_targets)].index, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Busco tiempo de entrega vs fecha de la opportunity\n",
    "(calculado en blocknum)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "outputs": [],
   "source": [
    "#df1['blocknum'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "outputs": [],
   "source": [
    "#dlivery_blocknum\n",
    "df1['early_delivery_blocknum'] = df1[\"Planned_Delivery_Start_Date\"].apply(lambda x: days_to(x))\n",
    "df2['early_delivery_blocknum'] = df2[\"Planned_Delivery_Start_Date\"].apply(lambda x: days_to(x))\n",
    "\n",
    "#df1['delivery_blocknum'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "outputs": [],
   "source": [
    "df1[\"delivery_distance\"] = df1.apply(lambda x: x['early_delivery_blocknum']-x[\"blocknum\"], axis=1)\n",
    "df2[\"delivery_distance\"] = df2.apply(lambda x: x['early_delivery_blocknum']-x[\"blocknum\"], axis=1)\n",
    "#df1['delivery_distance'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Delivery Window"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "outputs": [],
   "source": [
    "df1['late_delivery_blocknum'] = df1[\"Planned_Delivery_End_Date\"].apply(lambda x: days_to(x))\n",
    "df2['late_delivery_blocknum'] = df2[\"Planned_Delivery_End_Date\"].apply(lambda x: days_to(x))\n",
    "\n",
    "\n",
    "df1[\"delivery_window_blocknum\"] = df1.apply(lambda x: x['late_delivery_blocknum']-x[\"early_delivery_blocknum\"], axis=1)\n",
    "df2[\"delivery_window_blocknum\"] = df2.apply(lambda x: x['late_delivery_blocknum']-x[\"early_delivery_blocknum\"], axis=1)\n",
    "#df1['delivery_window_blocknum'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Last_Modified_Date"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "outputs": [],
   "source": [
    "df1['last_modified_blocknum'] = df1[\"Last_Modified_Date\"].apply(lambda x: days_to(x))\n",
    "df2['last_modified_blocknum'] = df2[\"Last_Modified_Date\"].apply(lambda x: days_to(x))\n",
    "\n",
    "\n",
    "df1[\"last_modified_to_deliver\"] = df1.apply(lambda x: x['early_delivery_blocknum']-x[\"last_modified_blocknum\"], axis=1)\n",
    "df2[\"last_modified_to_deliver\"] = df2.apply(lambda x: x['early_delivery_blocknum']-x[\"last_modified_blocknum\"], axis=1)\n",
    "# df1['last_modified_to_deliver'].describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Columnas que descartan closed won"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brand\n",
      "Product_Type\n",
      "Size\n",
      "Product_Category_B\n",
      "Price\n",
      "Currency\n",
      "FIN...\n"
     ]
    }
   ],
   "source": [
    "#TODOS LOS QUE TIENEN ESTAAS COLUMNAS SON LOST\n",
    "for column in df1.columns:\n",
    "    if np.sum(df1[df1['Stage']=='Closed Won'][column].value_counts())==0:\n",
    "        print(column)\n",
    "print(\"FIN...\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "outputs": [
    {
     "data": {
      "text/plain": "False    16023\nTrue       924\ndtype: int64"
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lost_condition_df1 = (~df1['Brand'].isna() | ~df1['Product_Type'].isna() | ~df1['Size'].isna()\\\n",
    "                 | ~df1['Product_Category_B'].isna() | ~df1['Currency'].isna() | ~df1['Price'].isna())\n",
    "lost_condition_df2 = (~df2['Brand'].isna() | ~df2['Product_Type'].isna() | ~df2['Size'].isna()\\\n",
    "                 | ~df2['Product_Category_B'].isna() | ~df2['Currency'].isna() | ~df2['Price'].isna())\n",
    "lost_condition_df1.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "outputs": [],
   "source": [
    "df1['lost'] = lost_condition_df1\n",
    "df2['lost'] = lost_condition_df2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "outputs": [
    {
     "data": {
      "text/plain": "Source                       0.560394\nBilling_Country              0.001593\nAccount_Type                 0.006609\nBrand                        0.974686\nProduct_Type                 0.970673\nSize                         0.965422\nProduct_Category_B           0.970732\nPrice                        0.978993\nCurrency                     0.947188\nQuote_Expiry_Date            0.272910\nASP                          0.189355\nASP_(converted)              0.189355\nPlanned_Delivery_End_Date    0.004426\nTotal_Amount                 0.003481\nlate_delivery_blocknum       0.004426\ndelivery_window_blocknum     0.004426\ndtype: float64"
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hypotesis: the features that contains more than 75% of NaN values \n",
    "#            do not contribute sustancial inforation\n",
    "\n",
    "\n",
    "na_values_rate = df1.isna().sum()/len(df1)\n",
    "na_values_rate = na_values_rate[na_values_rate>0]\n",
    "na_values_rate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns 'Brand', 'Product_Type', 'Size', 'Product_Category_B', 'Price', 'Currency' dropped\n"
     ]
    }
   ],
   "source": [
    "# Drop the most empty features\n",
    "to_drop = na_values_rate[na_values_rate > 0.75].index.to_list()\n",
    "\n",
    "trainset = df1.drop(columns = to_drop)\n",
    "testset = df2.drop(columns= to_drop)\n",
    "\n",
    "text = \"', '\".join(to_drop)\n",
    "print(f\"Columns '{text}' dropped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este modelo entonces se factorizan las columnas ``'Brand'``, ``'Product_Type'``, ``'Size'``, ``'Product_Category_B'``, ``'Price'``, ``'Currency'`` a una sola ``'lost'``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Source                       0.560394\nBilling_Country              0.001593\nAccount_Type                 0.006609\nQuote_Expiry_Date            0.272910\nASP                          0.189355\nASP_(converted)              0.189355\nPlanned_Delivery_End_Date    0.004426\nTotal_Amount                 0.003481\nlate_delivery_blocknum       0.004426\ndelivery_window_blocknum     0.004426\ndtype: float64"
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The remaining features with na_values\n",
    "contains_na = na_values_rate[na_values_rate <= 0.75].index.to_list()\n",
    "na_values_rate[contains_na]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_fill_na(df,columns):\n",
    "\n",
    "    result = df.copy()\n",
    "    dtypes = result[columns].dtypes\n",
    "    cat = dtypes[dtypes == \"category\"].index.to_list()\n",
    "    not_cat = dtypes[dtypes != \"category\"].index.to_list()\n",
    "    \n",
    "    for col in cat:\n",
    "        if \"Other\" not in result[col].cat.categories:\n",
    "            result[col].cat.add_categories(\"Other\",inplace=True)\n",
    "        result[col].fillna(\"Other\",inplace= True)\n",
    "    \n",
    "    for col in not_cat:\n",
    "        result[col].fillna(result[col].mean(),inplace= True)\n",
    "\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = my_fill_na(trainset,contains_na)\n",
    "testset = my_fill_na(testset,contains_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restan valores nulos en trainset: False\n",
      "Restan valores nulos en testset: False\n"
     ]
    }
   ],
   "source": [
    "na_train = trainset.isna().sum() > 0\n",
    "na_test = testset.isna().sum() > 0\n",
    "\n",
    "print(f\"Restan valores nulos en trainset: {na_train.any()}\")\n",
    "print(f\"Restan valores nulos en testset: {na_test.any()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya filtramos las features con excesivos Nan values y rellenamos aquellas que su procentage de nan values es moderado, con valores predeterminados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición del target de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "             Stage  target\nID                        \n27761  Closed Lost       0\n27760   Closed Won       1\n27446   Closed Won       1\n16808  Closed Lost       0\n16805  Closed Lost       0\n16802  Closed Lost       0\n16799  Closed Lost       0\n27455   Closed Won       1\n24353  Closed Lost       0\n24355  Closed Lost       0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Stage</th>\n      <th>target</th>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>27761</th>\n      <td>Closed Lost</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>27760</th>\n      <td>Closed Won</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>27446</th>\n      <td>Closed Won</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16808</th>\n      <td>Closed Lost</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16805</th>\n      <td>Closed Lost</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16802</th>\n      <td>Closed Lost</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16799</th>\n      <td>Closed Lost</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>27455</th>\n      <td>Closed Won</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>24353</th>\n      <td>Closed Lost</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24355</th>\n      <td>Closed Lost</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[\"target\"] = (trainset[\"Stage\"] == \"Closed Won\").astype(int)\n",
    "trainset[[\"Stage\",\"target\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de valores categóricos\n",
    "\n",
    "En esta sección, lo que vamos a hacer es buscar valores categóricos de los features del set de datos de prueba que no hayan sido contemplados en el set de entrenamiento, que por lo consiguiente, el modelo de machine learning los va a desconocer.\n",
    "\n",
    "Una vez identificados, los reemplazaremos con un valor genérico para \"otros valores\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'Andorra', 'Finland', 'Mongolia'}"
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_A = set(testset[\"Territory\"].value_counts().index)\n",
    "set_B = set(trainset[\"Territory\"].value_counts().index)\n",
    "for value in set_B: set_A.discard(value)\n",
    "set_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['Q1', 'Q2', 'Q3', 'Q4']"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "['Q1', 'Q2', 'Q3', 'Q4']"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "official_values = trainset[\"Delivery_Quarter\"].cat.categories.to_list()\n",
    "test_values = testset[\"Delivery_Quarter\"].cat.categories.to_list()\n",
    "display(official_values)\n",
    "display(test_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded = empty+to_drop+[\"Stage\",\"Opportunity_Name\"]\n",
    "results = []\n",
    "for column in categorical_columns:\n",
    "    if column in excluded: continue\n",
    "    \n",
    "    official_values = trainset[column].cat.categories.to_list()\n",
    "    test_values = testset[column].cat.categories.to_list()\n",
    "    \n",
    "    other_values = set(test_values)\n",
    "    for value in official_values: other_values.discard(value)\n",
    "    \n",
    "    if len(other_values)>0:\n",
    "        \n",
    "        if not \"Other\" in testset[column].cat.categories.to_list():\n",
    "            testset[column].cat.add_categories(\"Other\",inplace=True)\n",
    "        \n",
    "        if not \"Other\" in trainset[column].cat.categories.to_list():\n",
    "            trainset[column].cat.add_categories(\"Other\",inplace=True)\n",
    "            \n",
    "        testset[column].replace({x:\"Other\" for x in other_values},inplace=True)\n",
    "    \n",
    "        results.append((column,len(other_values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "              column  discarted values\n0          Territory                 3\n1    Billing_Country                 3\n2       Account_Name               205\n3  Opportunity_Owner                 5\n4   Last_Modified_By                 8\n5     Product_Family                20\n6       Product_Name                50",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>column</th>\n      <th>discarted values</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Territory</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Billing_Country</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Account_Name</td>\n      <td>205</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Opportunity_Owner</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Last_Modified_By</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Product_Family</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Product_Name</td>\n      <td>50</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "others_df = pd.DataFrame(results,columns= [\"column\", \"discarted values\"])\n",
    "others_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "count      7.00\nmean      42.00\nstd       73.81\nmin        3.00\n25%        4.00\n50%        8.00\n75%       35.00\nmax      205.00\nName: discarted values, dtype: float64"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total test values discarted 294, %0.2 of total test data\n"
     ]
    }
   ],
   "source": [
    "display(round(others_df[\"discarted values\"].describe(),2))\n",
    "print(f\"\\nTotal test values discarted {others_df['discarted values'].sum()}, \",end=\"\")\n",
    "print(f\"%{100*round(others_df['discarted values'].sum()/testset.size,3)} of total test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La información descartada es muy poca con respecto al volumen del set. Más adelante veremos si es posible extraer información de esto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding \n",
    "Realizaremos un dataset básico para poder correr el modelo por primera vez y observar los resultados.\n",
    "Luego realizaremos mejoras e ingeniería de features para ver como se comporta el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El set de entrenamiento tiene 847350 elementos\n",
      "El set de test tiene 122448 elementos\n"
     ]
    }
   ],
   "source": [
    "print(f\"El set de entrenamiento tiene {trainset.size} elementos\")\n",
    "print(f\"El set de test tiene {testset.size} elementos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features categóricos a encodear\n"
     ]
    },
    {
     "data": {
      "text/plain": "['Region',\n 'ASP_(converted)_Currency',\n 'Territory',\n 'Total_Taxable_Amount_Currency',\n 'Opportunity_Owner',\n 'Account_Owner',\n 'Delivery_Quarter',\n 'Quote_Type',\n 'Account_Name',\n 'Product_Family',\n 'ASP_Currency',\n 'Account_Type',\n 'Opportunity_Type',\n 'Delivery_Terms',\n 'Total_Amount_Currency',\n 'Bureaucratic_Code',\n 'Source ',\n 'Last_Modified_By',\n 'Product_Name',\n 'Billing_Country']"
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "excluded = empty+to_drop\n",
    "excluded.append(\"Opportunity_Name\")\n",
    "excluded.append(\"Stage\")\n",
    "excluded.append(\"target\")\n",
    "toEncode = set([col if col not in excluded else \"\" for col in categorical_columns])\n",
    "toEncode.discard(\"\")\n",
    "toEncode = list(toEncode)\n",
    "\n",
    "print(\"Features categóricos a encodear\")\n",
    "toEncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (16947, 2717)\n",
      "Size: 46044999\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16947 entries, 0 to 16946\n",
      "Columns: 2717 entries, 0 to 2716\n",
      "dtypes: float64(2717)\n",
      "memory usage: 351.3 MB\n"
     ]
    }
   ],
   "source": [
    "enc = OneHotEncoder(drop='if_binary')\n",
    "enc.fit(trainset[toEncode])\n",
    "ohed = pd.DataFrame(enc.transform(trainset[toEncode]).toarray())\n",
    "print(f\"Shape: {ohed.shape}\")\n",
    "print(f\"Size: {ohed.size}\")\n",
    "ohed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decidimos abandonar esta propuesta debido a la poca escalabilidad que tiene este método. Debido a las numerosas columnas categóricas con numerosos valores posibles cada una, necesitamos un total de ``2.723`` columnas para `16.947` entradas, lo cual nos deja un total de `46.146.681` valores en nuestra tabla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary encoding \n",
    "\n",
    "Este método nos permite encodear las features categóricas reduciendo considerablemente el dataset. Si suponemos que los 16.947 valores categóricos posibles se distribuyen uniformemente en las 7 columnas, entonces necesitaremos ``⌊log_2(16.947/7)⌋ + 1  = 9`` columnas nuevas por cada columna categórica original del dataset. En total ``9*7 = 63`` una propuesta cuestionable pero mucho mejor que la anterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class BinaryEncoding():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.__encodings = {}\n",
    "        \n",
    "    def __make_encoding(self,name,categories):\n",
    "\n",
    "        encoding = {}\n",
    "        n_cols = int(math.log(len(categories),2))+1\n",
    "        cols_names = [ name+\"_\"+str(x) for x in range(n_cols) ]\n",
    "\n",
    "        for i in range(len(categories)):\n",
    "            encoding[categories[i]] = list(f'{bin(i)[2:]}'.zfill(n_cols))\n",
    "\n",
    "        return (encoding,cols_names)\n",
    "    \n",
    "    def encode_Series(self, serie, name,verbose=False):\n",
    "        \n",
    "        \n",
    "        if not name in self.__encodings:\n",
    "\n",
    "            categories = serie.cat.categories.to_list()\n",
    "            self.__encodings[name] =  self.__make_encoding(name,categories)\n",
    "        \n",
    "        encoding,cols_names = self.__encodings[name]\n",
    "        data = []\n",
    "        indexs = []\n",
    "        \n",
    "        for index, value in serie.items():\n",
    "\n",
    "            data.append(encoding[value])\n",
    "            indexs.append(index)\n",
    "\n",
    "        df_result = pd.DataFrame(data,columns=cols_names,index=indexs)\n",
    "        return df_result,cols_names\n",
    "    \n",
    "    def getEncoding(self):\n",
    "        return self.__encodings\n",
    "    \n",
    "    def encode_DataFrame(self, df, toEncode, verbose = False):\n",
    "\n",
    "        full_encoded,columns = self.encode_Series(df[toEncode[0]],toEncode[0],verbose)\n",
    "       \n",
    "        for col in toEncode[1:]:\n",
    "            \n",
    "            encoding,col_names = self.encode_Series(df[col],col,verbose)\n",
    "            full_encoded[col_names] = encoding\n",
    "\n",
    "        if verbose: evaluate_encoding(df[toEncode],full_encoded)\n",
    "\n",
    "        return full_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def encode_serie(serie,name):\n",
    "    \n",
    "    encoding = {}\n",
    "    categories = serie.cat.categories.to_list()\n",
    "    n_cols = int(math.log(len(categories),2))+1\n",
    "    cols_names = [ name+\"_\"+str(x) for x in range(n_cols) ]\n",
    "\n",
    "    for i in range(len(categories)):\n",
    "        encoding[categories[i]] = list(f'{bin(i)[2:]}'.zfill(n_cols))\n",
    "    \n",
    "    data = []\n",
    "    indexs = []\n",
    "    \n",
    "    for index, value in serie.items():\n",
    "        data.append(encoding[value])\n",
    "        indexs.append(index)\n",
    "        \n",
    "    df_result = pd.DataFrame(data,columns=cols_names,index=indexs)\n",
    "    return df_result, cols_names\n",
    "\"\"\"\n",
    "[]+[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_encoding(original,encoded):\n",
    "\n",
    "    # Recuento de cada una de las combinaciones de \n",
    "    # la lista de features categóricos sin encodear\n",
    "    count1 = original.value_counts().to_frame()[0].values\n",
    "\n",
    "    # Recuento de cada una de las combinaciones de \n",
    "    # la lista de features categóricos ENCODEADOS\n",
    "    count2 = encoded.value_counts().to_frame()[0].values\n",
    "\n",
    "    # Comparación\n",
    "    print(\"El encoding fue realizado correctamente: \", np.equal(count1,count2).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def BinaryEncoding(df,toEncode,verbose = False):\n",
    "    \n",
    "    print(\"Shape prev\",df[toEncode].shape)\n",
    "    \n",
    "    full_encoded,columns = encode_serie(df[toEncode[0]],toEncode[0])\n",
    "    for col in toEncode[1:]:\n",
    "        encoding,col_names = encode_serie(df[col],col)\n",
    "        full_encoded[col_names] = encoding\n",
    "        \n",
    "    if verbose: evaluate_encoding(df[toEncode],full_encoded)\n",
    "        \n",
    "    return full_encoded\n",
    "\"\"\"\n",
    "[]+[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El encoding fue realizado correctamente:  True\n",
      "El encoding fue realizado correctamente:  True\n"
     ]
    }
   ],
   "source": [
    "encoder = BinaryEncoding()\n",
    "trainEncoded = encoder.encode_DataFrame(trainset,toEncode,verbose=True)\n",
    "testEncoded = encoder.encode_DataFrame(testset,toEncode,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-encoded shape: (16947, 97)\n",
      "Train-encoded size: 1643859\n",
      "--------------------------------\n",
      "Test-encoded shape: (2551, 97)\n",
      "Test-encoded size: 247447\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train-encoded shape: {trainEncoded.shape}\")\n",
    "print(f\"Train-encoded size: {trainEncoded.size}\")\n",
    "print(\"--------------------------------\")\n",
    "print(f\"Test-encoded shape: {testEncoded.shape}\")\n",
    "print(f\"Test-encoded size: {testEncoded.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pudimos encodear las columnas categóricas reduciendo notablemente el espacio. El cálculo de columnas utilizadas previo valía con la suposición de que todas las features tenían la misma cantidad de valores. Pero aún no cumpliendose, la diferencia de columnas es poca con respecto al resultado obtenido con One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminamos este procesamiento de datos, para ver como un modelo de RandomForest se comporta frente a esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 21 features no categóricos\n"
     ]
    },
    {
     "data": {
      "text/plain": "['Pricing, Delivery_Terms_Quote_Appr',\n 'Pricing, Delivery_Terms_Approved',\n 'Bureaucratic_Code_0_Approval',\n 'Bureaucratic_Code_0_Approved',\n 'Opportunity_ID',\n 'ASP',\n 'ASP_(converted)',\n 'Month',\n 'Delivery_Year',\n 'TRF',\n 'Total_Amount',\n 'Total_Taxable_Amount',\n 'blocknum',\n 'Occur',\n 'early_delivery_blocknum',\n 'delivery_distance',\n 'late_delivery_blocknum',\n 'delivery_window_blocknum',\n 'last_modified_blocknum',\n 'last_modified_to_deliver',\n 'lost']"
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_cols = list()\n",
    "categorical_columns.append(\"target\")\n",
    "for col in trainset.columns.to_list():\n",
    "    if not col in categorical_columns and not col in date_columns:\n",
    "        final_cols.append(col)\n",
    "print(f\"Selected {len(final_cols)} features no categóricos\")\n",
    "final_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(16947, 119)"
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalTrain = trainset.loc[:,final_cols+[\"target\"]]\n",
    "finalTrain[trainEncoded.columns.to_list()] = trainEncoded\n",
    "finalTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(2551, 118)"
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalTest = testset.loc[:,final_cols]\n",
    "finalTest[testEncoded.columns.to_list()] = testEncoded\n",
    "finalTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se eliminaron las columnas vacías o con valores iguales\n",
    "# Se rellenaron nan_values con promedios para features numéricos y con \"others\" para categóricos\n",
    "# Se eliminaron valores categóricos del set de test que no estan en el set de entrenamiento\n",
    "# Se realizó binary encoding para todos las features categóricas\n",
    "# Se agregó la columna \"target\" la cual tiene 1/0 según es \"Closed Won\" o no\n",
    "# Se eliminó la columna \"Stage\"\n",
    "\n",
    "path = \"datasets/\"\n",
    "name = \"xgb\"\n",
    "finalTrain.to_csv(path+name+\"-train.csv\", index = False)\n",
    "finalTest.to_csv(path+name+\"-test.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}